{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "from efficientnet.model import EfficientNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters:\n",
    "use_gpu = torch.cuda.is_available()\n",
    "in_channel=3\n",
    "class_num=2\n",
    "lr=0.001\n",
    "batch_size=32\n",
    "data_dir = 'dataset_genero'\n",
    "num_epochs=10\n",
    "input_size = 224\n",
    "momentum = 0.9\n",
    "net_name = 'efficientnet-b0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(data_dir, batch_size, set_name, shuffle):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            #transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [set_name]}\n",
    "    # num_workers=0 if CPU else =1\n",
    "    dataset_loaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      shuffle=shuffle, num_workers=1) for x in [set_name]}\n",
    "    data_set_sizes = len(image_datasets[set_name])\n",
    "    return dataset_loaders, data_set_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_ft, criterion, optimizer, lr_scheduler, num_epochs=50):\n",
    "    train_loss = []\n",
    "    since = time.time()\n",
    "    best_model_wts = model_ft.state_dict()\n",
    "    best_acc = 0.0\n",
    "    model_ft.train(True)\n",
    "    for epoch in range(num_epochs):\n",
    "        dset_loaders, dset_sizes = loaddata(data_dir=data_dir, batch_size=batch_size, set_name='train', shuffle=True)\n",
    "        print('Data Size', dset_sizes)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        optimizer = lr_scheduler(optimizer, epoch)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        count = 0\n",
    "\n",
    "        for data in dset_loaders['train']:\n",
    "            inputs, labels = data\n",
    "            labels = torch.squeeze(labels.type(torch.LongTensor))\n",
    "            if use_gpu:\n",
    "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            outputs = model_ft(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            count += 1\n",
    "            if count % 30 == 0 or outputs.size()[0] < batch_size:\n",
    "                print('Epoch:{}: loss:{:.3f}'.format(epoch, loss.item()))\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / dset_sizes\n",
    "        epoch_acc = running_corrects.double() / dset_sizes\n",
    "\n",
    "        print('Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "            epoch_loss, epoch_acc))\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = model_ft.state_dict()\n",
    "        if epoch_acc > 0.999:\n",
    "            break\n",
    "\n",
    "    # save best model\n",
    "    save_dir = data_dir + '/model'\n",
    "    model_ft.load_state_dict(best_model_wts)\n",
    "    model_out_path = save_dir + \"/\" + net_name + '.pth'\n",
    "    torch.save(model_ft, model_out_path)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return train_loss, best_model_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    cont = 0\n",
    "    outPre = []\n",
    "    outLabel = []\n",
    "    dset_loaders, dset_sizes = loaddata(data_dir=data_dir, batch_size=16, set_name='test', shuffle=False)\n",
    "    for data in dset_loaders['test']:\n",
    "        inputs, labels = data\n",
    "        labels = torch.squeeze(labels.type(torch.LongTensor))\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if cont == 0:\n",
    "            outPre = outputs.data.cpu()\n",
    "            outLabel = labels.data.cpu()\n",
    "        else:\n",
    "            outPre = torch.cat((outPre, outputs.data.cpu()), 0)\n",
    "            outLabel = torch.cat((outLabel, labels.data.cpu()), 0)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        cont += 1\n",
    "    print('Loss: {:.4f} Acc: {:.4f}'.format(running_loss / dset_sizes,\n",
    "                                            running_corrects.double() / dset_sizes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_lr_scheduler(optimizer, epoch, init_lr=0.01, lr_decay_epoch=10):\n",
    "    \"\"\"Decay learning rate by a f#            model_out_path =\"./model/W_epoch_{}.pth\".format(epoch)\n",
    "#            torch.save(model_W, model_out_path) actor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (0.8**(epoch // lr_decay_epoch))\n",
    "    print('LR is set to {}'.format(lr))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "pth_map = {\n",
    "    'efficientnet-b0': 'efficientnet-b0-355c32eb.pth',\n",
    "    'efficientnet-b1': 'efficientnet-b1-f1951068.pth',\n",
    "    'efficientnet-b2': 'efficientnet-b2-8bb594d6.pth',\n",
    "    'efficientnet-b3': 'efficientnet-b3-5fb5a3c3.pth',\n",
    "    'efficientnet-b4': 'efficientnet-b4-6ed6700e.pth',\n",
    "    'efficientnet-b5': 'efficientnet-b5-b6417697.pth',\n",
    "    'efficientnet-b6': 'efficientnet-b6-c76e70fd.pth',\n",
    "    'efficientnet-b7': 'efficientnet-b7-dcc49843.pth',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_ft = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "net_name=\"efficientnet-b0\"\n",
    "model_ft = EfficientNet.from_name(net_name)\n",
    "net_weight = 'efficientnet/' + pth_map[net_name]\n",
    "state_dict = torch.load(net_weight)\n",
    "model_ft.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size 24621\n",
      "Epoch 0/9\n",
      "----------\n",
      "LR is set to 0.01\n",
      "Epoch:0: loss:0.676\n",
      "Epoch:0: loss:0.498\n",
      "Epoch:0: loss:0.498\n",
      "Epoch:0: loss:0.473\n",
      "Epoch:0: loss:0.526\n",
      "Epoch:0: loss:0.318\n",
      "Epoch:0: loss:0.510\n",
      "Epoch:0: loss:0.535\n",
      "Epoch:0: loss:0.390\n",
      "Epoch:0: loss:0.644\n",
      "Epoch:0: loss:0.522\n",
      "Epoch:0: loss:0.556\n",
      "Epoch:0: loss:0.679\n",
      "Epoch:0: loss:0.427\n",
      "Epoch:0: loss:0.391\n",
      "Epoch:0: loss:0.444\n",
      "Epoch:0: loss:0.332\n",
      "Epoch:0: loss:0.375\n",
      "Epoch:0: loss:0.412\n",
      "Epoch:0: loss:0.495\n",
      "Epoch:0: loss:0.328\n",
      "Epoch:0: loss:0.602\n",
      "Epoch:0: loss:0.551\n",
      "Epoch:0: loss:0.359\n",
      "Epoch:0: loss:0.390\n",
      "Epoch:0: loss:0.559\n",
      "Loss: 0.4990 Acc: 0.7522\n",
      "Data Size 24621\n",
      "Epoch 1/9\n",
      "----------\n",
      "LR is set to 0.01\n",
      "Epoch:1: loss:0.318\n",
      "Epoch:1: loss:0.355\n",
      "Epoch:1: loss:0.465\n",
      "Epoch:1: loss:0.382\n",
      "Epoch:1: loss:0.438\n",
      "Epoch:1: loss:0.252\n",
      "Epoch:1: loss:0.272\n",
      "Epoch:1: loss:0.398\n",
      "Epoch:1: loss:0.378\n",
      "Epoch:1: loss:0.299\n",
      "Epoch:1: loss:0.390\n",
      "Epoch:1: loss:0.232\n",
      "Epoch:1: loss:0.426\n",
      "Epoch:1: loss:0.171\n",
      "Epoch:1: loss:0.460\n",
      "Epoch:1: loss:0.409\n",
      "Epoch:1: loss:0.354\n",
      "Epoch:1: loss:0.367\n",
      "Epoch:1: loss:0.235\n",
      "Epoch:1: loss:0.290\n",
      "Epoch:1: loss:0.202\n",
      "Epoch:1: loss:0.452\n",
      "Epoch:1: loss:0.549\n",
      "Epoch:1: loss:0.141\n",
      "Epoch:1: loss:0.273\n",
      "Epoch:1: loss:0.305\n",
      "Loss: 0.3565 Acc: 0.8392\n",
      "Data Size 24621\n",
      "Epoch 2/9\n",
      "----------\n",
      "LR is set to 0.01\n",
      "Epoch:2: loss:0.243\n",
      "Epoch:2: loss:0.159\n",
      "Epoch:2: loss:0.479\n",
      "Epoch:2: loss:0.256\n",
      "Epoch:2: loss:0.298\n",
      "Epoch:2: loss:0.247\n",
      "Epoch:2: loss:0.203\n",
      "Epoch:2: loss:0.440\n",
      "Epoch:2: loss:0.289\n",
      "Epoch:2: loss:0.375\n",
      "Epoch:2: loss:0.355\n",
      "Epoch:2: loss:0.299\n",
      "Epoch:2: loss:0.198\n",
      "Epoch:2: loss:0.319\n",
      "Epoch:2: loss:0.216\n",
      "Epoch:2: loss:0.216\n",
      "Epoch:2: loss:0.519\n",
      "Epoch:2: loss:0.210\n",
      "Epoch:2: loss:0.416\n",
      "Epoch:2: loss:0.356\n",
      "Epoch:2: loss:0.317\n",
      "Epoch:2: loss:0.401\n",
      "Epoch:2: loss:0.271\n",
      "Epoch:2: loss:0.329\n",
      "Epoch:2: loss:0.331\n",
      "Epoch:2: loss:0.214\n",
      "Loss: 0.2857 Acc: 0.8753\n",
      "Data Size 24621\n",
      "Epoch 3/9\n",
      "----------\n",
      "LR is set to 0.01\n",
      "Epoch:3: loss:0.195\n",
      "Epoch:3: loss:0.205\n",
      "Epoch:3: loss:0.114\n",
      "Epoch:3: loss:0.118\n",
      "Epoch:3: loss:0.302\n",
      "Epoch:3: loss:0.261\n",
      "Epoch:3: loss:0.176\n",
      "Epoch:3: loss:0.389\n",
      "Epoch:3: loss:0.359\n",
      "Epoch:3: loss:0.228\n",
      "Epoch:3: loss:0.231\n",
      "Epoch:3: loss:0.160\n",
      "Epoch:3: loss:0.143\n",
      "Epoch:3: loss:0.256\n",
      "Epoch:3: loss:0.227\n",
      "Epoch:3: loss:0.191\n",
      "Epoch:3: loss:0.127\n",
      "Epoch:3: loss:0.059\n",
      "Epoch:3: loss:0.117\n",
      "Epoch:3: loss:0.414\n",
      "Epoch:3: loss:0.329\n",
      "Epoch:3: loss:0.540\n",
      "Epoch:3: loss:0.411\n",
      "Epoch:3: loss:0.240\n",
      "Epoch:3: loss:0.235\n",
      "Epoch:3: loss:0.064\n",
      "Loss: 0.2344 Acc: 0.9017\n",
      "Data Size 24621\n",
      "Epoch 4/9\n",
      "----------\n",
      "LR is set to 0.01\n",
      "Epoch:4: loss:0.186\n",
      "Epoch:4: loss:0.467\n",
      "Epoch:4: loss:0.098\n",
      "Epoch:4: loss:0.354\n",
      "Epoch:4: loss:0.124\n",
      "Epoch:4: loss:0.188\n",
      "Epoch:4: loss:0.164\n",
      "Epoch:4: loss:0.353\n",
      "Epoch:4: loss:0.258\n",
      "Epoch:4: loss:0.283\n",
      "Epoch:4: loss:0.143\n",
      "Epoch:4: loss:0.088\n",
      "Epoch:4: loss:0.344\n",
      "Epoch:4: loss:0.365\n",
      "Epoch:4: loss:0.129\n",
      "Epoch:4: loss:0.124\n",
      "Epoch:4: loss:0.103\n",
      "Epoch:4: loss:0.336\n",
      "Epoch:4: loss:0.208\n",
      "Epoch:4: loss:0.138\n",
      "Epoch:4: loss:0.081\n",
      "Epoch:4: loss:0.080\n",
      "Epoch:4: loss:0.424\n",
      "Epoch:4: loss:0.112\n",
      "Epoch:4: loss:0.262\n",
      "Epoch:4: loss:0.390\n",
      "Loss: 0.1979 Acc: 0.9180\n",
      "Data Size 24621\n",
      "Epoch 5/9\n",
      "----------\n",
      "LR is set to 0.01\n",
      "Epoch:5: loss:0.082\n",
      "Epoch:5: loss:0.112\n",
      "Epoch:5: loss:0.084\n",
      "Epoch:5: loss:0.099\n",
      "Epoch:5: loss:0.554\n",
      "Epoch:5: loss:0.235\n",
      "Epoch:5: loss:0.146\n",
      "Epoch:5: loss:0.131\n",
      "Epoch:5: loss:0.100\n",
      "Epoch:5: loss:0.062\n",
      "Epoch:5: loss:0.132\n",
      "Epoch:5: loss:0.159\n",
      "Epoch:5: loss:0.210\n",
      "Epoch:5: loss:0.066\n",
      "Epoch:5: loss:0.070\n",
      "Epoch:5: loss:0.257\n",
      "Epoch:5: loss:0.122\n",
      "Epoch:5: loss:0.163\n",
      "Epoch:5: loss:0.191\n",
      "Epoch:5: loss:0.114\n",
      "Epoch:5: loss:0.083\n",
      "Epoch:5: loss:0.273\n",
      "Epoch:5: loss:0.057\n",
      "Epoch:5: loss:0.178\n",
      "Epoch:5: loss:0.189\n",
      "Epoch:5: loss:0.294\n",
      "Loss: 0.1731 Acc: 0.9293\n",
      "Data Size 24621\n",
      "Epoch 6/9\n",
      "----------\n",
      "LR is set to 0.01\n",
      "Epoch:6: loss:0.125\n",
      "Epoch:6: loss:0.038\n",
      "Epoch:6: loss:0.070\n",
      "Epoch:6: loss:0.243\n",
      "Epoch:6: loss:0.149\n",
      "Epoch:6: loss:0.028\n",
      "Epoch:6: loss:0.122\n",
      "Epoch:6: loss:0.274\n",
      "Epoch:6: loss:0.279\n",
      "Epoch:6: loss:0.105\n",
      "Epoch:6: loss:0.089\n",
      "Epoch:6: loss:0.070\n",
      "Epoch:6: loss:0.118\n",
      "Epoch:6: loss:0.266\n",
      "Epoch:6: loss:0.092\n",
      "Epoch:6: loss:0.068\n",
      "Epoch:6: loss:0.083\n",
      "Epoch:6: loss:0.279\n",
      "Epoch:6: loss:0.137\n",
      "Epoch:6: loss:0.090\n",
      "Epoch:6: loss:0.159\n",
      "Epoch:6: loss:0.336\n",
      "Epoch:6: loss:0.189\n",
      "Epoch:6: loss:0.216\n",
      "Epoch:6: loss:0.197\n",
      "Epoch:6: loss:0.116\n",
      "Loss: 0.1560 Acc: 0.9370\n",
      "Data Size 24621\n",
      "Epoch 7/9\n",
      "----------\n",
      "LR is set to 0.01\n",
      "Epoch:7: loss:0.073\n",
      "Epoch:7: loss:0.052\n",
      "Epoch:7: loss:0.056\n",
      "Epoch:7: loss:0.115\n",
      "Epoch:7: loss:0.191\n",
      "Epoch:7: loss:0.050\n",
      "Epoch:7: loss:0.089\n",
      "Epoch:7: loss:0.356\n",
      "Epoch:7: loss:0.151\n",
      "Epoch:7: loss:0.038\n",
      "Epoch:7: loss:0.071\n",
      "Epoch:7: loss:0.299\n",
      "Epoch:7: loss:0.074\n",
      "Epoch:7: loss:0.256\n",
      "Epoch:7: loss:0.194\n",
      "Epoch:7: loss:0.099\n",
      "Epoch:7: loss:0.081\n",
      "Epoch:7: loss:0.065\n",
      "Epoch:7: loss:0.065\n",
      "Epoch:7: loss:0.079\n",
      "Epoch:7: loss:0.162\n",
      "Epoch:7: loss:0.144\n",
      "Epoch:7: loss:0.021\n",
      "Epoch:7: loss:0.119\n",
      "Epoch:7: loss:0.072\n",
      "Epoch:7: loss:0.561\n",
      "Loss: 0.1395 Acc: 0.9433\n",
      "Data Size 24621\n",
      "Epoch 8/9\n",
      "----------\n",
      "LR is set to 0.01\n",
      "Epoch:8: loss:0.150\n",
      "Epoch:8: loss:0.133\n",
      "Epoch:8: loss:0.091\n",
      "Epoch:8: loss:0.241\n",
      "Epoch:8: loss:0.023\n",
      "Epoch:8: loss:0.054\n",
      "Epoch:8: loss:0.277\n",
      "Epoch:8: loss:0.116\n",
      "Epoch:8: loss:0.114\n",
      "Epoch:8: loss:0.088\n",
      "Epoch:8: loss:0.014\n",
      "Epoch:8: loss:0.256\n",
      "Epoch:8: loss:0.150\n",
      "Epoch:8: loss:0.183\n",
      "Epoch:8: loss:0.054\n",
      "Epoch:8: loss:0.160\n",
      "Epoch:8: loss:0.067\n",
      "Epoch:8: loss:0.168\n",
      "Epoch:8: loss:0.152\n",
      "Epoch:8: loss:0.047\n",
      "Epoch:8: loss:0.152\n",
      "Epoch:8: loss:0.069\n",
      "Epoch:8: loss:0.169\n",
      "Epoch:8: loss:0.073\n",
      "Epoch:8: loss:0.069\n",
      "Epoch:8: loss:0.187\n",
      "Loss: 0.1350 Acc: 0.9462\n",
      "Data Size 24621\n",
      "Epoch 9/9\n",
      "----------\n",
      "LR is set to 0.01\n",
      "Epoch:9: loss:0.073\n",
      "Epoch:9: loss:0.045\n",
      "Epoch:9: loss:0.023\n",
      "Epoch:9: loss:0.047\n",
      "Epoch:9: loss:0.060\n",
      "Epoch:9: loss:0.170\n",
      "Epoch:9: loss:0.038\n",
      "Epoch:9: loss:0.021\n",
      "Epoch:9: loss:0.672\n",
      "Epoch:9: loss:0.098\n",
      "Epoch:9: loss:0.042\n",
      "Epoch:9: loss:0.150\n",
      "Epoch:9: loss:0.105\n",
      "Epoch:9: loss:0.055\n",
      "Epoch:9: loss:0.138\n",
      "Epoch:9: loss:0.106\n",
      "Epoch:9: loss:0.185\n",
      "Epoch:9: loss:0.104\n",
      "Epoch:9: loss:0.053\n",
      "Epoch:9: loss:0.150\n",
      "Epoch:9: loss:0.069\n",
      "Epoch:9: loss:0.136\n",
      "Epoch:9: loss:0.386\n",
      "Epoch:9: loss:0.060\n",
      "Epoch:9: loss:0.041\n",
      "Epoch:9: loss:0.016\n",
      "Loss: 0.1256 Acc: 0.9504\n",
      "Training complete in 29m 6s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_ftrs = model_ft._fc.in_features\n",
    "model_ft._fc = nn.Linear(num_ftrs, class_num)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "optimizer = optim.SGD((model_ft.parameters()), lr=lr,\n",
    "                      momentum=momentum, weight_decay=0.0004)\n",
    "\n",
    "train_loss, best_model_wts = train_model(model_ft, criterion, optimizer, exp_lr_scheduler, num_epochs=num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 2.4121e-11]], device='cuda:0')\n",
      "-----\n",
      "f                                                                           (100.00%)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "PATH = \"resultado/model/efficientnet-b0.pth\"\n",
    "\n",
    "# Load\n",
    "model = torch.load(PATH)\n",
    "model.eval()\n",
    "\n",
    "# Create the preprocessing transformation here\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "def loaddata(data_dir, batch_size, set_name, shuffle):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "from PIL import Image\n",
    "# Preprocess image\n",
    "tfms = transforms.Compose([transforms.Resize(224), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),])\n",
    "img = tfms(Image.open('images/tijuana_aer.jpeg')).unsqueeze(0)\n",
    "#img = tfms(Image.open('dataset_genero/test/m/61d8cf0ad0dd825c2753b614.jpg')).unsqueeze(0)\n",
    "img= img.cuda()\n",
    "# Load ImageNet class names\n",
    "labels_map = [\"f\",\"m\"]\n",
    "\n",
    "# Classify\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(img)\n",
    "\n",
    "# Print predictions\n",
    "print(torch.softmax(outputs, dim=1))\n",
    "print('-----')\n",
    "\n",
    "for idx in torch.topk(outputs, k=1).indices.squeeze(0).tolist():\n",
    "    prob = torch.softmax(outputs, dim=1)[0, idx].item()\n",
    "    print('{label:<75} ({p:.2f}%)'.format(label=labels_map[idx], p=prob*100))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40c6efbe1bfe1230750b1c6b6ff43642b05be0357dd448c396fdd7cde68b7842"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('fa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
